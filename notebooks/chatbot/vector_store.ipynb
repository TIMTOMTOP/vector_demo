{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Creation for Document Search\n",
    "\n",
    "This notebook creates a searchable vector database from markdown documents using Pinecone and OpenAI embeddings. It processes the following steps:\n",
    "\n",
    "1. Chunks markdown documents into manageable segments\n",
    "2. Creates embeddings for each chunk using OpenAI's text-embedding-3-small model\n",
    "3. Stores these embeddings in a Pinecone vector database for efficient similarity search\n",
    "\n",
    "This setup enables semantic search across documents, allowing for natural language queries to find relevant content based on meaning rather than just keyword matching. The system is particularly useful for building AI-powered document search and retrieval systems.\n",
    "\n",
    "## Requirements:\n",
    "- OpenAI API key\n",
    "- Pinecone API key\n",
    "- Markdown documents in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Chunking Function\n",
    "Function to split text into overlapping chunks based on token count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_tokens(text: str, chunk_size=500, overlap=100, model_name=\"gpt-3.5-turbo\") -> list:\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    token_ids = encoding.encode(text)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(token_ids):\n",
    "        end = start + chunk_size\n",
    "        chunk_ids = token_ids[start:end]\n",
    "        chunk_text = encoding.decode(chunk_ids)\n",
    "        chunks.append(chunk_text)\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pinecone and OpenAI Clients\n",
    "Initialize the Pinecone and OpenAI clients using environment variables.\n",
    "Then define the passage loading function that reads markdown files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "client= OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def load_passages(data_dir=\".\"):\n",
    "    passages = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith('.md'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, data_dir)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        full_text = f.read()\n",
    "                    \n",
    "                    full_text=full_text.strip()\n",
    "\n",
    "                    chunked_passages = chunk_text_by_tokens(\n",
    "                        full_text, \n",
    "                        chunk_size=500, \n",
    "                        overlap=100, \n",
    "                        model_name=\"gpt-3.5-turbo\"\n",
    "                    )\n",
    "\n",
    "                    passages[relative_path] = chunked_passages\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(passages)} passages from {data_dir}\")\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings\n",
    "Load the markdown files and create embeddings for each chunk using OpenAI's embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 passages from data\n"
     ]
    }
   ],
   "source": [
    "data=load_passages()\n",
    "\n",
    "embedding_dict={}\n",
    "\n",
    "for key, value in data.items():\n",
    "    embedding_dict[key] = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=value\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pinecone Index\n",
    "Set up a new serverless Pinecone index with the appropriate dimensions for OpenAI embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"showcase-index\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    ")\n",
    "\n",
    "index=pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Vector Data\n",
    "Format the embeddings and metadata into vectors suitable for Pinecone storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "\n",
    "for file_path, file_embeddings in embedding_dict.items():\n",
    "    file_vectors = [\n",
    "        {\n",
    "            \"id\": f\"{file_path}_{i}\",  # Create unique ID combining file path and index\n",
    "            \"values\": embedding.embedding,\n",
    "            \"metadata\": {\n",
    "                \"source_file\": file_path,\n",
    "                \"text\":data[file_path][i]\n",
    "            }\n",
    "        }\n",
    "        for i, embedding in enumerate(file_embeddings.data)\n",
    "    ]\n",
    "    \n",
    "    # Extend our main vectors list with the vectors from this file\n",
    "    vectors.extend(file_vectors)\n",
    "\n",
    "print(f\"Created {len(vectors)} total vectors\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Vectors to Pinecone\n",
    "Upload all prepared vectors to the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 157 total vectors\n"
     ]
    }
   ],
   "source": [
    "index.upsert(vectors=vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
