{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomas/tool_box/venv/lib/python3.13/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "index_name = \"showcase-index\"\n",
    "\n",
    "pinecone_store = Pinecone.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based on the provided context.\n",
    "If the context doesn't contain the answer, say that you don't know.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=pinecone_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, male cats are more likely to be left-pawed.\n"
     ]
    }
   ],
   "source": [
    "def query_knowledge_base(question: str) -> str:\n",
    "    return qa_chain.invoke({\"query\": question})\n",
    "\n",
    "question = \"Are male cats more likely to be left-pawed?\"\n",
    "result = query_knowledge_base(question)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Initialize the models\n",
    "router_model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
    "general_model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "index_name = \"showcase-index\"\n",
    "\n",
    "pinecone_store = Pinecone.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create the prompt template for the cat knowledge base\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based on the provided context.\n",
    "If the context doesn't contain the answer, say that you don't know.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the cat knowledge QA chain\n",
    "cat_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=pinecone_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "# Define the routing function\n",
    "def route_question(state: MessagesState) -> Literal[\"cat_knowledge\", \"general_knowledge\"]:\n",
    "    messages = state['messages']\n",
    "    question = messages[-1].content\n",
    "    print(f\"Evaluating question: {question}\")\n",
    "    \n",
    "    # Ask the LLM to evaluate if this is a cat-related question\n",
    "    evaluation_prompt = f\"\"\"Determine if the following question is about cats/felines. \n",
    "    Question: {question}\n",
    "    Reply with just 'cat_knowledge' or 'general_knowledge'.\"\"\"\n",
    "    \n",
    "    response = router_model.invoke([HumanMessage(content=evaluation_prompt)])\n",
    "    route = \"cat_knowledge\" if \"cat_knowledge\" in response.content.lower() else \"general_knowledge\"\n",
    "    print(f\"Routing to: {route}\")\n",
    "    return route\n",
    "\n",
    "def handle_cat_question(state: MessagesState):\n",
    "    question = state['messages'][-1].content\n",
    "    print(f\"Handling cat question: {question}\")\n",
    "    result = cat_qa.invoke({\"query\": question})\n",
    "    return {\"messages\": [HumanMessage(content=str(result))]}\n",
    "\n",
    "def handle_general_question(state: MessagesState):\n",
    "    question = state['messages'][-1].content\n",
    "    print(f\"Handling general question: {question}\")\n",
    "    response = general_model.invoke([HumanMessage(content=question)])\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1251b0c20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"cat_knowledge\", handle_cat_question)\n",
    "workflow.add_node(\"general_knowledge\", handle_general_question)\n",
    "\n",
    "# Add conditional edges from start\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"cat_knowledge\": \"cat_knowledge\",\n",
    "        \"general_knowledge\": \"general_knowledge\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges to end\n",
    "workflow.add_edge(\"cat_knowledge\", END)\n",
    "workflow.add_edge(\"general_knowledge\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Cat Question ===\n",
      "Evaluating question: What do cats like to eat?\n",
      "Routing to: cat_knowledge\n",
      "Handling cat question: What do cats like to eat?\n",
      "{'query': 'What do cats like to eat?', 'result': 'Cats like to eat foods such as cat food, mice, and other small animals.'}\n",
      "\n",
      "=== Testing General Question ===\n",
      "Evaluating question: What is the capital of France?\n",
      "Routing to: general_knowledge\n",
      "Handling general question: What is the capital of France?\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Initialize memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "print(\"\\n=== Testing Cat Question ===\")\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What do cats like to eat?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"cat_test\"}}\n",
    ")\n",
    "print(final_state[\"messages\"][-1].content)\n",
    "\n",
    "print(\"\\n=== Testing General Question ===\")\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the capital of France?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"general_test\"}}\n",
    ")\n",
    "print(final_state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
